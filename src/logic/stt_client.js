/**
 * ì‹¤ì‹œê°„ STT ë°ëª¨ìš© í´ë¼ì´ì–¸íŠ¸ ëª¨ë“ˆ
 * - 0.01s ë¸”ë¡ ë‹¨ìœ„ë¡œ ëˆ„ì 
 * - ìµœê·¼ 0.5s í‰ê·  RMS < THRESHOLD ë©´ êµ¬ê°„ ì¢…ë£Œ
 * - ì¢…ë£Œ ì‹œ êµ¬ê°„ì„ WAV(16kHz/mono/16-bit)ë¡œ ì¸ì½”ë”©í•´ WSë¡œ ì „ì†¡
 * - ì„œë²„ëŠ” "WAV ë°”ì´íŠ¸ 1ê±´ ìˆ˜ì‹  â†’ í…ìŠ¤íŠ¸ 1ê±´ ë°˜í™˜" ë‹¨ë°œ WebSocket í”„ë¡œí† ì½œ ê°€ì •
 */

export class RealtimeSTTClient {
  /**
   * @param {{
   *   wsUrl: string,                     // Whisper WS ì„œë²„ (ì˜ˆ: "ws://114.110.128.184:31376/ws")
   *   audioNode?: AudioNode,             // ì´ë¯¸ ë§Œë“  AudioNode(ì˜ˆ: MediaElementAudioSourceNode). ì—†ìœ¼ë©´ ë§ˆì´í¬ ì‚¬ìš©
   *   onTranscript?: (text:string)=>void,// ì„œë²„ ì‘ë‹µ ìˆ˜ì‹  ì½œë°±
   *   onStatus?: (msg:string)=>void,     // ìƒíƒœ ë©”ì‹œì§€ ì½œë°±
   *   context?: AudioContext,            // ì¬ì‚¬ìš©í•  AudioContext (ì˜µì…˜)
   *   threshold?: number,                // RMS ì„ê³„ê°’ (ê¸°ë³¸ 0.015~0.03 ì‚¬ì´ë¶€í„° ì‹œì‘)
   *   blockMs?: number,                  // ë¸”ë¡ ê¸¸ì´(ms) ê¸°ë³¸ 10ms
   *   silenceWindowMs?: number,          // ë¬´ìŒ íŒì • ì°½(ms) ê¸°ë³¸ 500ms
   *   minSpeechMs?: number,              // ìµœì†Œ ë°œí™” ê¸¸ì´(ms) ê¸°ë³¸ 200ms
   *   maxSegmentMs?: number              // í•œ ì„¸ê·¸ë¨¼íŠ¸ ìµœëŒ€ ê¸¸ì´(ms) ê¸°ë³¸ 20_000
   * }} cfg
   */
  constructor(cfg) {
    this.wsUrl = cfg.wsUrl;
    this.onTranscript = cfg.onTranscript || (()=>{});
    this.onStatus = cfg.onStatus || (()=>{});

    // ====== VAD-like íŒŒë¼ë¯¸í„° ======
    this.threshold = cfg.threshold ?? 0.02;   // í”„ë¡œì íŠ¸/í™˜ê²½ì— ë”°ë¼ 0.015~0.03 ì¶”ì²œ
    this.blockMs = cfg.blockMs ?? 10;         // 0.01ì´ˆ
    this.silenceWindowMs = cfg.silenceWindowMs ?? 500; // 0.5ì´ˆ
    this.minSpeechMs = cfg.minSpeechMs ?? 200;
    this.maxSegmentMs = cfg.maxSegmentMs ?? 20_000;

    // ====== ì˜¤ë””ì˜¤/ì›Œí¬ë › ======
    this.ctx = cfg.context || new (window.AudioContext || window.webkitAudioContext)();
    this.audioNode = cfg.audioNode ?? null;   // ì—†ìœ¼ë©´ ë§ˆì´í¬ë¡œ ìƒì„±
    this.workletNode = null;

    // ëˆ„ì  ë²„í¼ë“¤
    this.currentSegment = [];   // Float32Array ì¡°ê°ë“¤ (ì›ìƒ˜í”Œë ˆì´íŠ¸)
    this.currentLength = 0;     // ìƒ˜í”Œ ê°œìˆ˜
    this.segmentStartTime = 0;

    // ë¸”ë¡/RMS ìœˆë„ìš°
    this.blockTargetSamples = Math.round(this.ctx.sampleRate * (this.blockMs / 1000));
    this.blockCarry = null;     // ë¸”ë¡ ê²½ê³„ ë³´ì •ìš©
    this.rmsWindow = [];        // ìµœê·¼ ë¸”ë¡ RMS ê¸°ë¡
    this.rmsWindowSize = Math.max(1, Math.round(this.silenceWindowMs / this.blockMs)); // 500/10=50

    // WS 1íšŒì„± ì—°ê²°(ë³´ë‚¼ ë•Œë§ˆë‹¤ ìƒˆë¡œ ì—´ê³  ë‹«ëŠ”ë‹¤) â€” ì„œë²„ê°€ ë‹¨ë°œ íŒŒì¼â†’í…ìŠ¤íŠ¸ ì‘ë‹µì´ë¯€ë¡œ
    this._wsBusy = false;

    // ìƒíƒœ
    this._running = false;
  }

  /** ì´ˆê¸°í™”: AudioWorklet ë¡œë“œ + ì…ë ¥ ì†ŒìŠ¤ í™•ë³´ */
  async init() {
    // ì›Œí¬ë › ë“±ë¡
    const workletUrl = new URL('./frame-capture-processor.js', import.meta.url);
    await this.ctx.audioWorklet.addModule(workletUrl);

    // ì…ë ¥ ì†ŒìŠ¤(ì—†ìœ¼ë©´ ë§ˆì´í¬)
    if (!this.audioNode) {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: { echoCancellation: false, noiseSuppression: false, autoGainControl: false }, video: false });
      const src = new MediaStreamAudioSourceNode(this.ctx, { mediaStream: stream });
      this.audioNode = src;
    }

    // ì›Œí¬ë › ë…¸ë“œ
    this.workletNode = new AudioWorkletNode(this.ctx, 'frame-capture-processor');
    this.workletNode.port.onmessage = (ev) => this._onAudioBlock(ev.data);

    // íŒŒí˜• ë“± ê¸°ì¡´ UIì— ì´ë¯¸ ì—°ê²°ë¼ ìˆë‹¤ë©´ â€˜ë³‘ë ¬ íƒ­â€™ìœ¼ë¡œ ë¶„ê¸°ë§Œ ê±¸ì–´ì¤€ë‹¤
    this.audioNode.connect(this.workletNode);
  }

  /** ì‹œì‘ */
  async start() {
    if (this._running) return;
    if (this.ctx.state === 'suspended') await this.ctx.resume();

    this._running = true;
    this.currentSegment = [];
    this.currentLength = 0;
    this.segmentStartTime = performance.now();
    this.rmsWindow = [];
    this.blockCarry = null;

    this.onStatus('ğŸ™ï¸ STT ë¶„í•  ì‹œì‘');
  }

  /** ì •ì§€(ë‚¨ì€ êµ¬ê°„ë„ ë§ˆë¬´ë¦¬ ì „ì†¡) */
  async stop() {
    if (!this._running) return;
    this._running = false;

    // ì§„í–‰ ì¤‘ ì„¸ê·¸ë¨¼íŠ¸ê°€ ìˆìœ¼ë©´ ë§ˆë¬´ë¦¬
    if (this.currentLength > 0) {
      await this._finalizeAndSend();
    }
    this.onStatus('ğŸ›‘ STT ë¶„í•  ì •ì§€');
  }

  // ===== ë‚´ë¶€ ë¡œì§ =====

  _onAudioBlock(floatBlock /* Float32Array at ctx.sampleRate */) {
    if (!this._running) return;

    // ë¸”ë¡ ê²½ê³„ 10msë¡œ ë§ì¶”ê¸° (ì›Œí¬ë › chunk í¬ê¸°ê°€ ê°€ë³€ì¼ ìˆ˜ ìˆìœ¼ë‹ˆ 0.01së¡œ ì¬ì¡°í•©)
    let src = floatBlock;
    if (this.blockCarry) {
      // carryì™€ í•©ì³ì„œ ì²˜ë¦¬
      const merged = new Float32Array(this.blockCarry.length + src.length);
      merged.set(this.blockCarry, 0);
      merged.set(src, this.blockCarry.length);
      src = merged;
      this.blockCarry = null;
    }

    let offset = 0;
    while (offset + this.blockTargetSamples <= src.length) {
      const block = src.subarray(offset, offset + this.blockTargetSamples);
      this._consumeFixedBlock(block);
      offset += this.blockTargetSamples;
    }

    // ë‚¨ëŠ” ì¡°ê°ì€ carryë¡œ ë³´ê´€
    if (offset < src.length) {
      this.blockCarry = src.subarray(offset).slice();
    }
  }

  _consumeFixedBlock(block /* Float32Array (exact 10ms) */) {
    // ëˆ„ì (ì› ìƒ˜í”Œë ˆì´íŠ¸ ê·¸ëŒ€ë¡œ)
    this.currentSegment.push(block);
    this.currentLength += block.length;

    // RMS ê³„ì‚°
    let sum = 0;
    for (let i = 0; i < block.length; i++) {
      const v = block[i];
      sum += v * v;
    }
    const rms = Math.sqrt(sum / block.length);

    // RMS ìœˆë„ìš° ì—…ë°ì´íŠ¸
    this.rmsWindow.push(rms);
    if (this.rmsWindow.length > this.rmsWindowSize) {
      this.rmsWindow.shift();
    }

    // ë¬´ìŒ íŒì • (ìµœê·¼ 0.5ì´ˆ í‰ê· )
    const avgRms = this.rmsWindow.reduce((a, b) => a + b, 0) / this.rmsWindow.length;

    const elapsedMs = performance.now() - this.segmentStartTime;
    const longEnough = elapsedMs >= this.minSpeechMs;
    const tooLong = elapsedMs >= this.maxSegmentMs;

    if ((longEnough && avgRms < this.threshold) || tooLong) {
      // êµ¬ê°„ ì¢…ë£Œ
      this._finalizeAndSend();
      // ë‹¤ìŒ ì„¸ê·¸ë¨¼íŠ¸ ì¤€ë¹„
      this.currentSegment = [];
      this.currentLength = 0;
      this.segmentStartTime = performance.now();
      this.rmsWindow = [];
    }
  }

  async _finalizeAndSend() {
    const srIn = this.ctx.sampleRate;
    const mono = this._concatFloat32(this.currentSegment, this.currentLength);
    // WAVëŠ” 16kHz/mono/16-bitë¡œ ë³´ëƒ„
    const wavBlob = await this._floatToWav16kMono(mono, srIn);

    try {
      this.onStatus(`ğŸ“¤ ì„¸ê·¸ë¨¼íŠ¸ ì „ì†¡ (${(wavBlob.size/1024).toFixed(1)} KB)`);
      const text = await this._postWavViaWebSocket(wavBlob);
      if (text && text.trim()) {
        this.onTranscript(text);
      } else {
        this.onStatus('â„¹ï¸ ì„œë²„ ì‘ë‹µì´ ë¹„ì–´ìˆìŒ');
      }
    } catch (e) {
      this.onStatus('âš ï¸ ì „ì†¡/ìˆ˜ì‹  ì‹¤íŒ¨: ' + (e?.message || e));
    }
  }

  _concatFloat32(chunks, totalLen) {
    const out = new Float32Array(totalLen);
    let off = 0;
    for (const c of chunks) {
      out.set(c, off);
      off += c.length;
    }
    return out;
  }

  async _floatToWav16kMono(floatData, srcRate) {
    // OfflineAudioContextë¡œ 16kHz ì¬ìƒ˜í”Œ
    const targetRate = 16000;
    if (srcRate === targetRate) {
      return this._encodeWavPCM16(floatData, targetRate);
    }

    // 1) src ë²„í¼ ì±„ìš°ê¸°
    const srcBuf = new AudioBuffer({ length: floatData.length, numberOfChannels: 1, sampleRate: srcRate });
    srcBuf.copyToChannel(floatData, 0, 0);

    // 2) ì˜¤í”„ë¼ì¸ ë Œë”ë¡œ ë¦¬ìƒ˜í”Œ
    const duration = srcBuf.length / srcRate;
    const framesOut = Math.ceil(duration * targetRate);
    const offline = new OfflineAudioContext(1, framesOut, targetRate);

    const srcNode = new AudioBufferSourceNode(offline, { buffer: srcBuf });
    srcNode.connect(offline.destination);
    srcNode.start();

    const rendered = await offline.startRendering();
    const out = new Float32Array(rendered.length);
    rendered.copyFromChannel(out, 0, 0);

    return this._encodeWavPCM16(out, targetRate);
  }

  _encodeWavPCM16(float32, sampleRate) {
    // Float32 [-1,1] -> int16
    const len = float32.length;
    const bytesPerSample = 2;
    const blockAlign = 1 * bytesPerSample;
    const byteRate = sampleRate * blockAlign;
    const dataSize = len * bytesPerSample;
    const headerSize = 44;
    const buffer = new ArrayBuffer(headerSize + dataSize);
    const view = new DataView(buffer);

    // RIFF header
    this._writeAscii(view, 0, 'RIFF');
    view.setUint32(4, 36 + dataSize, true);
    this._writeAscii(view, 8, 'WAVE');

    // fmt chunk
    this._writeAscii(view, 12, 'fmt ');
    view.setUint32(16, 16, true);           // Subchunk1Size (16 for PCM)
    view.setUint16(20, 1, true);            // AudioFormat (1 = PCM)
    view.setUint16(22, 1, true);            // NumChannels (mono)
    view.setUint32(24, sampleRate, true);   // SampleRate
    view.setUint32(28, byteRate, true);     // ByteRate
    view.setUint16(32, blockAlign, true);   // BlockAlign
    view.setUint16(34, 16, true);           // BitsPerSample

    // data chunk
    this._writeAscii(view, 36, 'data');
    view.setUint32(40, dataSize, true);

    // PCM samples
    let offset = 44;
    for (let i = 0; i < len; i++, offset += 2) {
      let s = Math.max(-1, Math.min(1, float32[i]));
      view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
    }

    return new Blob([buffer], { type: 'audio/wav' });
  }

  _writeAscii(view, offset, str) {
    for (let i = 0; i < str.length; i++) view.setUint8(offset + i, str.charCodeAt(i));
  }

  _postWavViaWebSocket(wavBlob) {
    return new Promise((resolve, reject) => {
      if (this._wsBusy) {
        // ë‹¨ë°œ ì„œë²„ ê°€ì •: ë™ì‹œ ì „ì†¡ ë°©ì§€
        return reject(new Error('WebSocket busy â€” ì´ì „ ìš”ì²­ ì²˜ë¦¬ ì¤‘'));
      }
      this._wsBusy = true;

      const ws = new WebSocket(this.wsUrl);
      ws.binaryType = 'arraybuffer';

      const timer = setTimeout(() => {
        try { ws.close(); } catch {}
        this._wsBusy = false;
        reject(new Error('WebSocket timeout'));
      }, 30_000); // 30s

      ws.onopen = async () => {
        try {
          const arr = await wavBlob.arrayBuffer();
          ws.send(arr);
        } catch (e) {
          clearTimeout(timer);
          this._wsBusy = false;
          try { ws.close(); } catch {}
          reject(e);
        }
      };

      ws.onerror = (ev) => {
        clearTimeout(timer);
        this._wsBusy = false;
        reject(new Error('WebSocket error'));
      };

      ws.onmessage = (ev) => {
        clearTimeout(timer);
        this._wsBusy = false;
        try { ws.close(); } catch {}

        // ì„œë²„ê°€ í…ìŠ¤íŠ¸ ë¬¸ìì—´ ë˜ëŠ” JSONì„ ë³´ë‚¼ ìˆ˜ ìˆë‹¤ê³  ê°€ì •
        if (typeof ev.data === 'string') {
          // ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
          resolve(ev.data);
        } else {
          // ArrayBuffer/Blobì´ë©´ ì‹œë„í•´ì„œ í…ìŠ¤íŠ¸ë¡œ
          if (ev.data instanceof Blob) {
            ev.data.text().then(resolve).catch(reject);
          } else if (ev.data instanceof ArrayBuffer) {
            const dec = new TextDecoder();
            resolve(dec.decode(new Uint8Array(ev.data)));
          } else {
            resolve('');
          }
        }
      };

      ws.onclose = () => {
        // onmessageì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë¨
      };
    });
  }
}
